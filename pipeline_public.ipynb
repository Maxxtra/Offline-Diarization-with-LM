{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d8d32a56-2d70-4da4-a3e1-0da47bb47fbc",
   "metadata": {},
   "source": [
    "Starting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de0b569-b5b4-471c-9f3b-ec99adc704e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07626097-5fba-40b8-8320-c3cf37ed15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import subprocess\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import whisper\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from whisper.audio import pad_or_trim, log_mel_spectrogram, N_FRAMES\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from pyannote.audio import Pipeline\n",
    "from openai import AzureOpenAI\n",
    "from datasets import load_dataset\n",
    "from pyannote.core import Annotation, Segment\n",
    "from pyannote.metrics.diarization import DiarizationPurity, DiarizationCoverage\n",
    "\n",
    "\n",
    "# from config.config import load_config\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "# config = load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa879d7f-e4d1-4e32-8109-eccd3d1ffb99",
   "metadata": {},
   "source": [
    "Pyannote class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af681642-d5f6-4b51-8cd9-b8596486a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyannoteProcessor:\n",
    "    \"\"\"\n",
    "    Class to perform speaker diarization using the Pyannote library.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pipeline = Pipeline.from_pretrained(\n",
    "           \"pyannote/speaker-diarization-3.1\",\n",
    "            use_auth_token=\"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\",\n",
    "        )\n",
    "\n",
    "    def perform_diarization(self, audio_file_path):\n",
    "        self.pipeline.to(torch.device('cuda')) # switch to gpu\n",
    "\n",
    "        # Hardcoding the number of speakers\n",
    "        diarization = self.pipeline(audio_file_path, num_speakers=2)\n",
    "\n",
    "        with open (\"sample.rttm\", \"w\") as rttm:\n",
    "          diarization.write_rttm(rttm)\n",
    "        \n",
    "\n",
    "    def rttm_to_dataframe(self, rttm_file_path):\n",
    "        columns = [\n",
    "            \"Type\",\n",
    "            \"File ID\",\n",
    "            \"Channel\",\n",
    "            \"Start Time\",\n",
    "            \"Duration\",\n",
    "            \"Orthography\",\n",
    "            \"Confidence\",\n",
    "            \"Speaker\",\n",
    "            \"x\",\n",
    "            \"y\",\n",
    "        ]\n",
    "        data = []\n",
    "\n",
    "        with open(rttm_file_path, \"r\") as rttm_file:\n",
    "            lines = rttm_file.readlines()\n",
    "\n",
    "        data = [line.strip().split() for line in lines]\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        df = df.drop([\"x\", \"y\", \"Orthography\", \"Confidence\"], axis=1)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559526a-cf1d-437a-860e-1efbace593d3",
   "metadata": {},
   "source": [
    "Whisper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96623d-a910-40a7-9dd6-27e7d4643ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperProcessor:\n",
    "    def __init__(self):\n",
    "        # Initialize the Whisper model\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(self.device)\n",
    "        self.model = whisper.load_model(\"large-v2\").to(self.device)\n",
    "        # torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "        # model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "        # model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        #     model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "        # )\n",
    "        # model.to(device)\n",
    "\n",
    "        # processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "        # pipe = pipeline(\n",
    "        #     \"automatic-speech-recognition\",\n",
    "        #     model=model,\n",
    "        #     tokenizer=processor.tokenizer,\n",
    "        #     feature_extractor=processor.feature_extractor,\n",
    "        #     max_new_tokens=128,\n",
    "        #     chunk_length_s=25,\n",
    "        #     batch_size=16,\n",
    "        #     torch_dtype=torch_dtype,\n",
    "        #     device=device,\n",
    "        # )\n",
    "\n",
    "\n",
    "    def transcribe_audio_with_whisper(self, audio_file, detected_language):\n",
    "        \"\"\"\n",
    "        Transcribes an audio segment using the Whisper ASR model.\n",
    "\n",
    "        Args:\n",
    "            audio_file (str): Path to the audio file.\n",
    "            detected_language (str): Detected language of the audio.\n",
    "\n",
    "        Returns:\n",
    "            dict: Transcription result containing text and other information.\n",
    "        \"\"\"\n",
    "        result = self.model.transcribe(\n",
    "            audio_file, language=detected_language, fp16=False, temperature = (0.8, 1.0)\n",
    "        )\n",
    "        return result\n",
    "\n",
    "        # Whisper predicts the language of the source audio automatically. \n",
    "        # If the source audio language is known a-priori, it can be passed as an argument to the pipeline:\n",
    "\n",
    "        # result = pipe(audio_file, generate_kwargs={\"language\": \"english\"})\n",
    "        # return result[\"text\"]\n",
    "\n",
    "    def detect_audio_language(self, audio) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Detects the language of an audio segment using the Whisper ASR model.\n",
    "\n",
    "        Args:\n",
    "            audio (AudioSegment): Audio segment to detect language from.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, float]: Detected language and confidence.\n",
    "        \"\"\"\n",
    "        mel_segment = pad_or_trim(log_mel_spectrogram(audio), N_FRAMES).to(\n",
    "            self.model.device\n",
    "        )\n",
    "        _, probs = self.model.detect_language(mel_segment)\n",
    "        detected_language = max(probs, key=probs.get)\n",
    "        confidence = probs[detected_language]\n",
    "\n",
    "        return detected_language, confidence\n",
    "\n",
    "    def process_audio_segment(\n",
    "        self, audio_file, start_time, end_time, detected_language\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Processes an audio segment within a specified time range.\n",
    "\n",
    "        Args:\n",
    "            audio_file (str): Path to the audio file.\n",
    "            start_time (int): Start time of the segment in milliseconds.\n",
    "            end_time (int): End time of the segment in milliseconds.\n",
    "            detected_language (str): Detected language of the audio segment.\n",
    "\n",
    "        Returns:\n",
    "            str: Collapsed transcript for the processed audio segment.\n",
    "        \"\"\"\n",
    "        start_time = float(start_time * 1000)\n",
    "        end_time = float(end_time * 1000)\n",
    "\n",
    "        audio = AudioSegment.from_file(audio_file)\n",
    "        audio_segment = audio[start_time:end_time]\n",
    "        \n",
    "        audio_segment_path = f\"audio_segment_{start_time}.wav\"\n",
    "        audio_segment.export(audio_segment_path, format=\"wav\")\n",
    "        \n",
    "        # Transcribe the audio segment\n",
    "        transcription_result = self.transcribe_audio_with_whisper(\n",
    "            audio_segment_path, detected_language\n",
    "        )\n",
    "        whisper_transcript = transcription_result[\"text\"]\n",
    "\n",
    "        # Split the transcript into segments\n",
    "        segments = whisper_transcript.split(\"\\n\")\n",
    "\n",
    "        # Collapse the segments\n",
    "        collapsed_transcript = self.collapse_segments(segments)\n",
    "\n",
    "        # Delete the temporary audio segment file\n",
    "        os.remove(audio_segment_path)\n",
    "\n",
    "        return collapsed_transcript\n",
    "\n",
    "    def collapse_segments(self, transcript_segments):\n",
    "        \"\"\"\n",
    "        Collapses individual words and spaces in the transcript segments.\n",
    "\n",
    "        Args:\n",
    "            transcript_segments (list): List of transcript segments.\n",
    "\n",
    "        Returns:\n",
    "            str: Collapsed transcript with words and spaces combined.\n",
    "        \"\"\"\n",
    "        segment_counter = 0\n",
    "        collapsed_segments = []\n",
    "\n",
    "        for segment in transcript_segments:\n",
    "            if segment.startswith(\"Segment\"):\n",
    "                segment_counter += 1\n",
    "                collapsed_segments.append(segment)\n",
    "            else:\n",
    "                words = segment.split()\n",
    "                for word in words:\n",
    "                    collapsed_segments.append(word)\n",
    "                    collapsed_segments.append(\" \")\n",
    "\n",
    "        collapsed_transcript = \"\".join(collapsed_segments)\n",
    "        return collapsed_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8fa06-a284-47e6-ac67-1ccedda75dc7",
   "metadata": {},
   "source": [
    "ChatGPT apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28e4ce-8e43-4281-b946-24cf3b844d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_GPT4 = AzureOpenAI(\n",
    "    azure_endpoint = \"https://dlcru-east-us2.openai.azure.com/\",\n",
    "    api_version = \"2023-03-15-preview\",\n",
    "    api_key = \"INSERT_YOUR_KEY\",\n",
    ")\n",
    "\n",
    "model_GPT4 = \"dlcru-gpt4\"\n",
    "\n",
    "\n",
    "client_GPT35_turbo = AzureOpenAI(\n",
    "    azure_endpoint =  \"https://gpt3test-dlcru.openai.azure.com/\",\n",
    "    api_version = \"2023-03-15-preview\",\n",
    "    api_key =  \"INSERT_YOUR_KEY\"\n",
    ")\n",
    "\n",
    "model_GPT35_turbo = \"dlcru-gpt-35-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09746888-2381-4487-bd4d-3311e58dd3ad",
   "metadata": {},
   "source": [
    "LLM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5972e-5440-4830-ab79-cb1984eb3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, client, model):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def create_prompt(self, text, shot_type):\n",
    "        prompt_zero_shot = f\"\"\"In the speaker diarization transcript below, some words are potentially misplaced.\n",
    "            Please correct those words and move them to the right speaker.\n",
    "            Directly show the corrected transcript without explaining what changes were made or why you\n",
    "            made those changes\n",
    "            \n",
    "            \"{text}\"\n",
    "            \"\"\"\n",
    "        prompt_one_shot = f\"\"\"In the speaker diarization transcript below, some words are potentially misplaced. There are only 2 speakers. \n",
    "            Please correct those words and move them to the right speaker. For example, given this input transcript,\n",
    "            <spk:1> How are you doing today? I <spk:2> am doing very well. How was everything at the\n",
    "            <spk:1> party? Oh, the party? It was awesome. We had lots of fun. Good <spk:2> to hear!\n",
    "            The correct output transcript should be:\n",
    "            <spk:1> How are you doing today? <spk:2> I am doing very well. How was everything at the\n",
    "            party? <spk:1> Oh, the party? It was awesome. We had lots of fun. <spk:2> Good to hear!\n",
    "            Now, please correct the transcript below.\\n\n",
    "            \n",
    "             \"{text}\"\n",
    "            \"\"\"\n",
    "        if shot_type == 'zero_shot':\n",
    "            return prompt_zero_shot\n",
    "        elif shot_type == 'one_shot':\n",
    "            return prompt_one_shot\n",
    "\n",
    "\n",
    "    def get_completion(self, text, shot_type, temperature=0):\n",
    "        prompt = self.create_prompt(text, shot_type)\n",
    "        \n",
    "        message_objects = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Answer shortly and only what you are asked.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                completion = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=message_objects,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                return completion.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                if '429' in str(e):  # Check if the error is due to rate limit (429 status code)\n",
    "                    retry_after = 9  # Retry after 9 seconds as per the error message\n",
    "                    print(f\"Rate limit exceeded. Retrying after {retry_after} seconds.\")\n",
    "                    time.sleep(retry_after)\n",
    "                else:\n",
    "                    raise  # Re-raise other exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8147d7c-3747-4b24-ac81-75cbad0c5534",
   "metadata": {},
   "source": [
    "The \"brain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737080c-17cd-4b26-a320-3dcfea4591be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    def __init__(self, pyannote_processor, whisper_processor):\n",
    "        self.pyannote = pyannote_processor\n",
    "        self.whisper = whisper_processor\n",
    "\n",
    "    def extract_speakers(self, text):\n",
    "        labels = []\n",
    "        speaker_count = {}\n",
    "        current_label = 1  # Start with label 1\n",
    "    \n",
    "        lines = text.strip().split(':')\n",
    "    \n",
    "        for line in lines:\n",
    "            speaker_id = line.split(':')[0].strip()\n",
    "            if speaker_id not in speaker_count:\n",
    "                speaker_count[speaker_id] = 1\n",
    "            else:\n",
    "                speaker_count[speaker_id] += 1\n",
    "    \n",
    "            labels.append(current_label)\n",
    "            current_label = 2 if current_label == 1 else 1  # Alternate between 1 and 2\n",
    "    \n",
    "        return labels\n",
    "\n",
    "    def process_and_append_to_csv(self, dataset, output_csv_path):\n",
    "        column_names = [\"audio_number\", \"text_GT\", \"diarization_without_LM\", \n",
    "                                                    \"diarization_35_turbo_zero_shot\", \"diarization_35_turbo_one_shot\",\n",
    "                                                    \"diarization_4_zero_shot\", \"diarization_4_one_shot\",\n",
    "                        \"speakers_GT\", \"speakers_without_LM\", \n",
    "                                                    \"speakers_35_turbo_zero_shot\", \"speakers_35_turbo_one_shot\",\n",
    "                                                    \"speakers_4_zero_shot\", \"speakers_4_one_shot\"]\n",
    "\n",
    "        with open(output_csv_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "            csv_writer = csv.writer(\n",
    "                csv_file, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL\n",
    "            )\n",
    "\n",
    "            if os.path.getsize(output_csv_path) == 0:\n",
    "                csv_writer.writerow(column_names)\n",
    "\n",
    "            for audio_number, row in enumerate(tqdm(dataset, desc=\"Processing dataset\")):\n",
    "                audio_data = row['audio']\n",
    "                speakers_GT = row['speakers']\n",
    "\n",
    "                # Remove consecutive repetitions from speakers_GT\n",
    "                filtered_GT = [speakers_GT[0]]  # Start with the first element\n",
    "\n",
    "                for i in range(1, len(speakers_GT)):\n",
    "                    if speakers_GT[i] != filtered_GT[-1]:  # If current element is different from the last added element\n",
    "                        filtered_GT.append(speakers_GT[i])\n",
    "                \n",
    "                                \n",
    "                audio_array = audio_data['array']\n",
    "                sampling_rate = audio_data['sampling_rate']\n",
    "                \n",
    "                # Normalize audio array to the range [-32768, 32767] (16-bit PCM range)\n",
    "                audio_array_normalized = np.int16(audio_array * 32767)\n",
    "                \n",
    "                # Specify the output WAV file path\n",
    "                output_wav_file = os.path.join(\n",
    "                    os.path.dirname(output_csv_path),\n",
    "                    os.path.splitext(output_csv_path)[0] + '_audio.wav'\n",
    "                )\n",
    "\n",
    "                sf.write(output_wav_file, audio_array, sampling_rate, subtype='PCM_16')\n",
    "\n",
    "\n",
    "                # Making the Ground Truth of the words with Whisper hardcoding the language\n",
    "                whisper_GT = self.whisper.transcribe_audio_with_whisper(\n",
    "                    output_wav_file, \"english\"\n",
    "                )\n",
    "\n",
    "                text_GT = whisper_GT[\"text\"]\n",
    "\n",
    "                try:\n",
    "                    # Perform Pyannote diarization\n",
    "                    print(\"Starting Pyannote...\")\n",
    "                    self.pyannote.perform_diarization(output_wav_file)\n",
    "                    print(\"Finished Pyannote\")\n",
    "\n",
    "                    rttm_file_path = \"sample.rttm\"\n",
    "                    df = self.pyannote.rttm_to_dataframe(rttm_file_path)\n",
    "                    df = df.astype({\"Start Time\": \"float\"})\n",
    "                    df = df.astype({\"Duration\": \"float\"})\n",
    "                    df[\"Utterance\"] = None\n",
    "                    df[\"End Time\"] = df[\"Start Time\"] + df[\"Duration\"]\n",
    "\n",
    "                    silence_gap_pairs = []\n",
    "\n",
    "                    for ind in df.index:\n",
    "                        start_time = df[\"Start Time\"][ind]\n",
    "                        end_time = df[\"End Time\"][ind]\n",
    "                        speaker = df[\"Speaker\"][ind]\n",
    "\n",
    "                        silence_gap_pairs.append((start_time, end_time, speaker))\n",
    "\n",
    "                    attributes_list = []\n",
    "\n",
    "                    current_start, current_end, current_speaker = silence_gap_pairs[0]\n",
    "\n",
    "                    for start, end, speaker in silence_gap_pairs[1:]:\n",
    "                        if speaker == current_speaker:\n",
    "                            current_end = end\n",
    "                        else:\n",
    "                            attributes_list.append((current_start, current_end, current_speaker, \"\"))\n",
    "                            current_start, current_end, current_speaker = start, end, speaker\n",
    "\n",
    "                    print(\"Starting Whisper...\")\n",
    "                    for i, (start, end, speaker, text) in enumerate(attributes_list):\n",
    "                        transcript = self.whisper.process_audio_segment(\n",
    "                            output_wav_file, start, end, \"english\"  # Replace \"english\" with detected language\n",
    "                        )\n",
    "                        attributes_list[i] = (start, end, speaker, transcript)\n",
    "\n",
    "                    print(\"Finished Whisper\")\n",
    "\n",
    "                    diarization_without_LM = \" \".join([f\"{speaker}: {text}\" for _, _, speaker, text in attributes_list])\n",
    "\n",
    "                    # call the LM to correct the diarization output\n",
    "                    print(\"Starting 3.5 turbo...\")\n",
    "                    llm_gtp35_turbo = LLM(client_GPT35_turbo, model_GPT35_turbo)\n",
    "                    \n",
    "                    print(\"zero shot\")\n",
    "                    diarization_35_turbo_zero_shot = llm_gtp35_turbo.get_completion(text=diarization_without_LM, shot_type='zero_shot')\n",
    "                    \n",
    "                    print(\"one shot\")\n",
    "                    diarization_35_turbo_one_shot = llm_gtp35_turbo.get_completion(text=diarization_without_LM, shot_type='one_shot')\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "                    print(\"Starting 4...\")\n",
    "                    llm_gpt4 = LLM(client_GPT4, model_GPT4)\n",
    "\n",
    "                    print(\"zero shot\")\n",
    "                    diarization_4_zero_shot = llm_gpt4.get_completion(text=diarization_without_LM, shot_type='zero_shot')\n",
    "\n",
    "                    print(\"one shot\")\n",
    "                    diarization_4_one_shot = llm_gpt4.get_completion(text=diarization_without_LM, shot_type='one_shot')\n",
    "\n",
    "\n",
    "\n",
    "                    # compute the speakers for DiarizationPurity and DiarizationCoverage \n",
    "                    speakers_without_LM = self.extract_speakers(diarization_without_LM)\n",
    "                    speakers_35_turbo_zero_shot = self.extract_speakers(diarization_35_turbo_zero_shot)\n",
    "                    speakers_35_turbo_one_shot = self.extract_speakers(diarization_35_turbo_one_shot)\n",
    "                    speakers_4_zero_shot = self.extract_speakers(diarization_4_zero_shot)\n",
    "                    speakers_4_one_shot = self.extract_speakers(diarization_4_one_shot)\n",
    "            \n",
    "\n",
    "\n",
    "                    \n",
    "                    csv_writer.writerow([audio_number, text_GT, \n",
    "                                                                diarization_without_LM, \n",
    "                                                                diarization_35_turbo_zero_shot, diarization_35_turbo_one_shot,\n",
    "                                                                diarization_4_zero_shot, diarization_4_one_shot,\n",
    "                                                        filtered_GT,\n",
    "                                                                speakers_without_LM,\n",
    "                                                                speakers_35_turbo_zero_shot, speakers_35_turbo_one_shot,\n",
    "                                                                speakers_4_zero_shot, speakers_4_one_shot\n",
    "                                        ])\n",
    "\n",
    "                    # THIS ONLY FOR THE DEMO\n",
    "                    # THIS ONLY FOR THE DEMO\n",
    "                    # THIS ONLY FOR THE DEMO\n",
    "                    # THIS ONLY FOR THE DEMO\n",
    "                    break\n",
    "\n",
    "                    \n",
    "                    os.remove(output_wav_file)\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "                    # break\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred for audio {audio_number}: {e}\")\n",
    "\n",
    "                # Flush the buffer to the file\n",
    "                csv_file.flush()\n",
    "\n",
    "    def process_dataset(self, output_csv_path):\n",
    "        ds = load_dataset(\"talkbank/callhome\", \"eng\")\n",
    "        dataset = ds['data']\n",
    "\n",
    "        self.process_and_append_to_csv(dataset, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e68156-1884-4e20-a20c-2633b26e5be6",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e466fd-c233-47fd-bad1-d822c972e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "!gpustat\n",
    "\n",
    "diarization = PyannoteProcessor()\n",
    "stt = WhisperProcessor()\n",
    "audio_processor = AudioProcessor(diarization, stt)\n",
    "\n",
    "audio_processor.process_dataset(\"/root/diarizare/transcripts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa065c-e78e-4be5-9968-4516dfa98256",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd4e0b-f53b-45e8-9cea-423e439052b8",
   "metadata": {},
   "source": [
    "Word metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1cedd-2d71-463e-a765-bcaee654171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordMetrics:\n",
    "    @staticmethod\n",
    "    def preprocess_transcript(transcript):\n",
    "        # Define regex pattern to match speaker labels\n",
    "        pattern = r'SPEAKER_\\d+:'\n",
    "        \n",
    "        # Split transcript using regex pattern\n",
    "        segments = re.split(pattern, transcript)\n",
    "        \n",
    "        # Clean up segments (remove empty strings and leading/trailing spaces)\n",
    "        segments = [seg.strip() for seg in segments if seg.strip()]\n",
    "        \n",
    "        # Extract speaker labels\n",
    "        speaker_labels = re.findall(pattern, transcript)\n",
    "        \n",
    "        return segments, speaker_labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_metrics(transcript_GT, transcript_hypothesis):\n",
    "        # Preprocess ground truth and hypothesis transcripts\n",
    "        segments_GT, speakers_GT = WordMetrics.preprocess_transcript(transcript_GT)\n",
    "        segments_hypothesis, speakers_hypothesis = WordMetrics.preprocess_transcript(transcript_hypothesis)\n",
    "        \n",
    "        # Calculate WDER\n",
    "        def calculate_WDER(segments_GT, segments_hypothesis, speakers_GT, speakers_hypothesis):\n",
    "            SIS = 0\n",
    "            CIS = 0\n",
    "            S = 0\n",
    "            C = 0\n",
    "            \n",
    "            # Determine the maximum length to iterate over\n",
    "            max_length = min(len(segments_GT), len(segments_hypothesis), len(speakers_GT), len(speakers_hypothesis))\n",
    "            \n",
    "            for i in range(max_length):\n",
    "                # Skip if indices are out of range\n",
    "                if i >= len(segments_GT) or i >= len(segments_hypothesis) or i >= len(speakers_GT) or i >= len(speakers_hypothesis):\n",
    "                    continue\n",
    "                \n",
    "                speaker_h = speakers_hypothesis[i]\n",
    "                speaker_g = speakers_GT[i]\n",
    "                \n",
    "                words_h = segments_hypothesis[i].split()\n",
    "                words_g = segments_GT[i].split()\n",
    "                \n",
    "                # Count substitutions\n",
    "                for wh, wg in zip(words_h, words_g):\n",
    "                    if wh != wg:\n",
    "                        S += 1\n",
    "                        if speaker_h != speaker_g:\n",
    "                            SIS += 1\n",
    "                    else:\n",
    "                        C += 1\n",
    "                        if speaker_h != speaker_g:\n",
    "                            CIS += 1\n",
    "            \n",
    "            if (S + C) > 0:\n",
    "                WDER = (SIS + CIS) / (S + C)\n",
    "            else:\n",
    "                WDER = 0.0\n",
    "            \n",
    "            return WDER\n",
    "        \n",
    "        # Calculate cpWER\n",
    "        def calculate_cpWER(segments_GT, segments_hypothesis):\n",
    "            def compute_wer(ref, hyp):\n",
    "                # Function to compute Word Error Rate (WER)\n",
    "                ref_words = ref.split()\n",
    "                hyp_words = hyp.split()\n",
    "    \n",
    "                # Create a matrix to store edits\n",
    "                edits = [[0] * (len(hyp_words) + 1) for _ in range(len(ref_words) + 1)]\n",
    "    \n",
    "                # Initialize the first row and column\n",
    "                for i in range(len(ref_words) + 1):\n",
    "                    edits[i][0] = i\n",
    "                for j in range(len(hyp_words) + 1):\n",
    "                    edits[0][j] = j\n",
    "    \n",
    "                # Fill the matrix\n",
    "                for i in range(1, len(ref_words) + 1):\n",
    "                    for j in range(1, len(hyp_words) + 1):\n",
    "                        if ref_words[i - 1] == hyp_words[j - 1]:\n",
    "                            edits[i][j] = edits[i - 1][j - 1]\n",
    "                        else:\n",
    "                            substitute = edits[i - 1][j - 1] + 1\n",
    "                            insert = edits[i][j - 1] + 1\n",
    "                            delete = edits[i - 1][j] + 1\n",
    "                            edits[i][j] = min(substitute, insert, delete)\n",
    "    \n",
    "                return edits[len(ref_words)][len(hyp_words)] / len(ref_words)\n",
    "    \n",
    "            # Concatenate all segments for reference and hypothesis\n",
    "            ref_concatenated = ' '.join(segments_GT)\n",
    "            hyp_concatenated = ' '.join(segments_hypothesis)\n",
    "    \n",
    "            # Compute WER for all permutations\n",
    "            cpWER = compute_wer(ref_concatenated, hyp_concatenated)\n",
    "    \n",
    "            return cpWER\n",
    "        \n",
    "        # Compute WDER\n",
    "        WDER = calculate_WDER(segments_GT, segments_hypothesis, speakers_GT, speakers_hypothesis)\n",
    "        \n",
    "        # Compute cpWER\n",
    "        cpWER = calculate_cpWER(segments_GT, segments_hypothesis)\n",
    "        \n",
    "        return WDER, cpWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6e644-5385-467c-8dd3-ffb1302ae7c3",
   "metadata": {},
   "source": [
    "Speaker metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94951d-6d72-4fcc-9ec0-60316ab03432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakersMetrics:\n",
    "    @staticmethod\n",
    "    def convert_to_annotation(speaker_labels):\n",
    "        annotation = Annotation()  # Assuming Annotation class is defined elsewhere\n",
    "        current_speaker = None\n",
    "        current_start = None\n",
    "        \n",
    "        for i, speaker_label in enumerate(speaker_labels):\n",
    "            if i == 0 or speaker_label != speaker_labels[i - 1]:\n",
    "                # End previous segment\n",
    "                if current_speaker is not None:\n",
    "                    annotation[Segment(current_start, i)] = current_speaker\n",
    "                # Start new segment\n",
    "                current_speaker = speaker_label\n",
    "                current_start = i\n",
    "        \n",
    "        # Add last segment\n",
    "        if current_speaker is not None:\n",
    "            annotation[Segment(current_start, len(speaker_labels))] = current_speaker\n",
    "        \n",
    "        return annotation\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_metrics(reference_labels, hypothesis_labels):\n",
    "        # Convert speaker labels to annotations\n",
    "        reference_annotation = SpeakersMetrics.convert_to_annotation(reference_labels)\n",
    "        hypothesis_annotation = SpeakersMetrics.convert_to_annotation(hypothesis_labels)\n",
    "        \n",
    "        # Initialize purity and coverage metrics\n",
    "        purity = DiarizationPurity()  # Assuming DiarizationPurity and DiarizationCoverage are defined elsewhere\n",
    "        coverage = DiarizationCoverage()\n",
    "        \n",
    "        # Compute metrics\n",
    "        purity_score = purity(reference_annotation, hypothesis_annotation)\n",
    "        coverage_score = coverage(reference_annotation, hypothesis_annotation)\n",
    "        \n",
    "        return purity_score, coverage_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392999be-5eba-4fa7-9a6a-8630d9f15ef6",
   "metadata": {},
   "source": [
    "Compute all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ea70f-ccc6-4755-9077-d41b1debcad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to compute all metrics and update DataFrame\n",
    "def compute_all_metrics(df):\n",
    "    for suffix in [\"without_LM\", \"35_turbo_zero_shot\", \"35_turbo_one_shot\", \"4_zero_shot\", \"4_one_shot\"]:\n",
    "        # Compute WDER and cpWER using WordMetrics\n",
    "        df[f\"WDER_{suffix}\"], df[f\"cpWER_{suffix}\"] = zip(*df.apply(lambda row: WordMetrics.compute_metrics(row['text_GT'], row[f'diarization_{suffix}']), axis=1))\n",
    "        \n",
    "        # Compute DiarizationPurity and DiarizationCoverage using SpeakersMetrics\n",
    "        df[f\"DiarizationPurity_{suffix}\"], df[f\"DiarizationCoverage_{suffix}\"] = zip(*df.apply(lambda row: SpeakersMetrics.compute_metrics(row['speakers_GT'], row[f'speakers_{suffix}']), axis=1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load transcripts.csv into a pandas DataFrame\n",
    "df = pd.read_csv('/root/diarizare/transcripts.csv')\n",
    "\n",
    "# Compute all metrics and update DataFrame\n",
    "df = compute_all_metrics(df)\n",
    "\n",
    "# Save the updated DataFrame with metrics columns\n",
    "df.to_csv('/root/diarizare/transcripts_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae5978-e71c-41dc-aedd-9050db8487b0",
   "metadata": {},
   "source": [
    "Compute all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795d507-b28f-4e37-8ff5-014c54183b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/root/diarizare/transcripts_with_metrics.csv')\n",
    "\n",
    "# Define the metrics and methods\n",
    "metrics = [\"WDER\", \"cpWER\", \"DiarizationPurity\", \"DiarizationCoverage\"]\n",
    "methods = [\"without_LM\", \"35_turbo_zero_shot\", \"35_turbo_one_shot\", \"4_zero_shot\", \"4_one_shot\"]\n",
    "\n",
    "# Initialize an empty dictionary to store summary data\n",
    "summary_data = {}\n",
    "\n",
    "# Calculate the mean for each metric for each method\n",
    "for metric in metrics:\n",
    "    summary_data[metric] = [df[f\"{metric}_{method}\"].mean() for method in methods]\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame(summary_data, index=methods)\n",
    "\n",
    "# Print the summary table\n",
    "print(summary_df)\n",
    "\n",
    "# Optionally, save the summary DataFrame to a CSV file\n",
    "summary_df.to_csv('summary_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b5bee-5a2f-474e-8aa3-80c1301962fb",
   "metadata": {},
   "source": [
    "Extract information about cpWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792ca01-a750-4bab-87a4-18201b6e0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the CSV file\n",
    "file_path = 'transcripts_with_metrics.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the cpWER columns for each method\n",
    "cpWER_columns = [\"cpWER_35_turbo_zero_shot\", \"cpWER_35_turbo_one_shot\", \"cpWER_4_zero_shot\", \"cpWER_4_one_shot\"]\n",
    "\n",
    "# Find the minimum and maximum cpWER values and the corresponding columns\n",
    "min_cpWER = df[cpWER_columns].min().min()\n",
    "max_cpWER = df[cpWER_columns].max().max()\n",
    "min_cpWER_column = df[cpWER_columns].min().idxmin()\n",
    "max_cpWER_column = df[cpWER_columns].max().idxmax()\n",
    "\n",
    "# Find the rows with the minimum and maximum cpWER values\n",
    "min_cpWER_row = df[df[min_cpWER_column] == min_cpWER]\n",
    "max_cpWER_row = df[df[max_cpWER_column] == max_cpWER]\n",
    "\n",
    "# Print the relevant diarization details for the lowest cpWER\n",
    "print(\"Details for diarization without LM and the diarization with the lowest cpWER:\")\n",
    "print(\"Diarization without LM:\")\n",
    "print(min_cpWER_row[['audio_number', 'diarization_without_LM', 'cpWER_without_LM']])\n",
    "\n",
    "print(f\"Diarization with the lowest cpWER ({min_cpWER_column}):\")\n",
    "print(min_cpWER_row[['audio_number', min_cpWER_column.replace('cpWER', 'diarization'), min_cpWER_column]])\n",
    "\n",
    "# Print the relevant diarization details for the highest cpWER\n",
    "print(\"\\nDetails for diarization without LM and the diarization with the highest cpWER:\")\n",
    "print(\"Diarization without LM:\")\n",
    "print(max_cpWER_row[['audio_number', 'diarization_without_LM', 'cpWER_without_LM']])\n",
    "\n",
    "print(f\"Diarization with the highest cpWER ({max_cpWER_column}):\")\n",
    "print(max_cpWER_row[['audio_number', max_cpWER_column.replace('cpWER', 'diarization'), max_cpWER_column]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d428b1-dbeb-4567-9772-15bdc17b7044",
   "metadata": {},
   "source": [
    "Extract information about Diarization Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0274ad-d9d3-4c3e-8187-7209ab9f3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DiarizationPurity columns for each method\n",
    "DiarizationPurity_columns = [\"DiarizationPurity_35_turbo_zero_shot\", \"DiarizationPurity_35_turbo_one_shot\", \"DiarizationPurity_4_zero_shot\", \"DiarizationPurity_4_one_shot\"]\n",
    "\n",
    "# Find the minimum and maximum DiarizationPurity values and the corresponding columns\n",
    "min_DiarizationPurity = df[DiarizationPurity_columns].min().min()\n",
    "max_DiarizationPurity = df[DiarizationPurity_columns].max().max()\n",
    "min_DiarizationPurity_column = df[DiarizationPurity_columns].min().idxmin()\n",
    "max_DiarizationPurity_column = df[DiarizationPurity_columns].max().idxmax()\n",
    "\n",
    "# Find the rows with the minimum and maximum DiarizationPurity values\n",
    "min_DiarizationPurity_row = df[df[min_DiarizationPurity_column] == min_DiarizationPurity]\n",
    "max_DiarizationPurity_row = df[df[max_DiarizationPurity_column] == max_DiarizationPurity]\n",
    "\n",
    "# Print the relevant diarization details for the lowest DiarizationPurity\n",
    "print(\"Details for diarization without LM and the diarization with the lowest DiarizationPurity:\")\n",
    "print(\"Diarization without LM:\")\n",
    "print(min_DiarizationPurity_row[['audio_number', 'diarization_without_LM', 'DiarizationPurity_without_LM']])\n",
    "\n",
    "print(f\"Diarization with the lowest DiarizationPurity ({min_DiarizationPurity_column}):\")\n",
    "print(min_DiarizationPurity_row[['audio_number', min_DiarizationPurity_column.replace('DiarizationPurity', 'diarization'), min_DiarizationPurity_column]])\n",
    "\n",
    "# Print the relevant diarization details for the highest DiarizationPurity\n",
    "print(\"\\nDetails for diarization without LM and the diarization with the highest DiarizationPurity:\")\n",
    "print(\"Diarization without LM:\")\n",
    "print(max_DiarizationPurity_row[['audio_number', 'diarization_without_LM', 'DiarizationPurity_without_LM']])\n",
    "\n",
    "print(f\"Diarization with the highest DiarizationPurity ({max_DiarizationPurity_column}):\")\n",
    "print(max_DiarizationPurity_row[['audio_number', max_DiarizationPurity_column.replace('DiarizationPurity', 'diarization'), max_DiarizationPurity_column]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ae7c5-3c97-43f6-9c8d-aaa0fc27a50a",
   "metadata": {},
   "source": [
    "Extract information about Diarization Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7734140-4bea-4ac4-ac25-d18380bdc19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the CSV file\n",
    "file_path = 'transcripts_with_metrics.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the DiarizationCoverage columns for each method\n",
    "DiarizationCoverage_columns = [\"DiarizationCoverage_35_turbo_zero_shot\", \"DiarizationCoverage_35_turbo_one_shot\", \"DiarizationCoverage_4_zero_shot\", \"DiarizationCoverage_4_one_shot\"]\n",
    "\n",
    "# Find the minimum and maximum DiarizationCoverage values and the corresponding columns\n",
    "min_DiarizationCoverage = df[DiarizationCoverage_columns].min().min()\n",
    "max_DiarizationCoverage = df[DiarizationCoverage_columns].max().max()\n",
    "min_DiarizationCoverage_column = df[DiarizationCoverage_columns].min().idxmin()\n",
    "max_DiarizationCoverage_column = df[DiarizationCoverage_columns].max().idxmax()\n",
    "\n",
    "# Find the rows with the minimum and maximum DiarizationCoverage values\n",
    "min_DiarizationCoverage_row = df[df[min_DiarizationCoverage_column] == min_DiarizationCoverage]\n",
    "max_DiarizationCoverage_row = df[df[max_DiarizationCoverage_column] == max_DiarizationCoverage]\n",
    "\n",
    "# Print the relevant diarization details for the lowest DiarizationCoverage\n",
    "print(\"Details for diarization without LM and the diarization with the lowest DiarizationCoverage:\")\n",
    "print(\"Diarization without LM:\")\n",
    "print(min_DiarizationCoverage_row[['audio_number', 'diarization_without_LM', 'DiarizationCoverage_without_LM']])\n",
    "\n",
    "print(f\"Diarization with the lowest DiarizationCoverage ({min_DiarizationCoverage_column}):\")\n",
    "print(min_DiarizationCoverage_row[['audio_number', min_DiarizationCoverage_column.replace('DiarizationCoverage', 'diarization'), min_DiarizationCoverage_column]])\n",
    "\n",
    "# Print the relevant diarization details for the highest DiarizationCoverage\n",
    "print(\"\\nDetails for diarization without LM and the diarization with the highest DiarizationCoverage:\")\n",
    "print(\"Diarization without LM:\")\n",
    "print(max_DiarizationCoverage_row[['audio_number', 'diarization_without_LM', 'DiarizationCoverage_without_LM']])\n",
    "\n",
    "print(f\"Diarization with the highest DiarizationCoverage ({max_DiarizationCoverage_column}):\")\n",
    "print(max_DiarizationCoverage_row[['audio_number', max_DiarizationCoverage_column.replace('DiarizationCoverage', 'diarization'), max_DiarizationCoverage_column]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b69482-3759-4747-93f4-00f64b5492dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print(\"%s minutes\" % elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
